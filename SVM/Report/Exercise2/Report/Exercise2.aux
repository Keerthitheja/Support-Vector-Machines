\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Exercise 2: Function Estimation and Time Series Prediction}{15}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Support vector machine for function estimation}{15}{section.2.1}\protected@file@percent }
\newlabel{fig:lin_E=0.1_B=0.1}{{2.1a}{15}{$\epsilon = 0.1$, Bound = 0.1 \relax }{figure.caption.29}{}}
\newlabel{sub@fig:lin_E=0.1_B=0.1}{{a}{15}{$\epsilon = 0.1$, Bound = 0.1 \relax }{figure.caption.29}{}}
\newlabel{fig:lin_E=0.3_B=0.1}{{2.1b}{15}{$\epsilon = 0.3$, Bound = 0.1 \relax }{figure.caption.29}{}}
\newlabel{sub@fig:lin_E=0.3_B=0.1}{{b}{15}{$\epsilon = 0.3$, Bound = 0.1 \relax }{figure.caption.29}{}}
\newlabel{fig:lin_E=0.5_B=0.1}{{2.1c}{15}{$\epsilon = 0.5$, Bound = 0.1 \relax }{figure.caption.29}{}}
\newlabel{sub@fig:lin_E=0.5_B=0.1}{{c}{15}{$\epsilon = 0.5$, Bound = 0.1 \relax }{figure.caption.29}{}}
\newlabel{fig:lin_E=0.15_B=0.01}{{2.1d}{15}{$\epsilon = 0.15$, Bound = 0.01 \relax }{figure.caption.29}{}}
\newlabel{sub@fig:lin_E=0.15_B=0.01}{{d}{15}{$\epsilon = 0.15$, Bound = 0.01 \relax }{figure.caption.29}{}}
\newlabel{fig:lin_E=0.15_B=0.1}{{2.1e}{15}{$\epsilon = 0.15$, Bound = 0.1 \relax }{figure.caption.29}{}}
\newlabel{sub@fig:lin_E=0.15_B=0.1}{{e}{15}{$\epsilon = 0.15$, Bound = 0.1 \relax }{figure.caption.29}{}}
\newlabel{fig:lin_E=0.15_B=10}{{2.1f}{15}{$\epsilon = 0.15$, Bound = 10 \relax }{figure.caption.29}{}}
\newlabel{sub@fig:lin_E=0.15_B=10}{{f}{15}{$\epsilon = 0.15$, Bound = 10 \relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces SVM for Regression using Linear Kernel: demo \texttt  {uiregress}. All the figures have same data points and performance of the regressor for different values of $\epsilon $ and bound are observed\relax }}{15}{figure.caption.29}\protected@file@percent }
\newlabel{fig:uiregress_Linear}{{2.1}{15}{SVM for Regression using Linear Kernel: demo \texttt {uiregress}. All the figures have same data points and performance of the regressor for different values of $\epsilon $ and bound are observed\relax }{figure.caption.29}{}}
\newlabel{fig:lin_E=0.6_B=inf}{{2.2a}{16}{Linear : $\epsilon = 0.6$, Bound = inf \relax }{figure.caption.30}{}}
\newlabel{sub@fig:lin_E=0.6_B=inf}{{a}{16}{Linear : $\epsilon = 0.6$, Bound = inf \relax }{figure.caption.30}{}}
\newlabel{fig:Poly_deg=3_E=1_B=0.6}{{2.2b}{16}{Poly : degree =3, $\epsilon = 1$, Bound = 0.6 \relax }{figure.caption.30}{}}
\newlabel{sub@fig:Poly_deg=3_E=1_B=0.6}{{b}{16}{Poly : degree =3, $\epsilon = 1$, Bound = 0.6 \relax }{figure.caption.30}{}}
\newlabel{fig:Poly_deg=5_E=1_B=0.6}{{2.2c}{16}{Poly : degree =5, $\epsilon = 1$, Bound = 0.6 \relax }{figure.caption.30}{}}
\newlabel{sub@fig:Poly_deg=5_E=1_B=0.6}{{c}{16}{Poly : degree =5, $\epsilon = 1$, Bound = 0.6 \relax }{figure.caption.30}{}}
\newlabel{fig:RBF_sig=0.05_E=1_B=0.6}{{2.2d}{16}{RBF : $\sigma ^2 = 0.05$ $\epsilon = 0.15$, Bound = 0.01 \relax }{figure.caption.30}{}}
\newlabel{sub@fig:RBF_sig=0.05_E=1_B=0.6}{{d}{16}{RBF : $\sigma ^2 = 0.05$ $\epsilon = 0.15$, Bound = 0.01 \relax }{figure.caption.30}{}}
\newlabel{fig:RBF_sig=0.1_E=1_B=0.6}{{2.2e}{16}{RBF : $\sigma ^2 = 0.1$ $\epsilon = 0.15$, Bound = 0.01 \relax }{figure.caption.30}{}}
\newlabel{sub@fig:RBF_sig=0.1_E=1_B=0.6}{{e}{16}{RBF : $\sigma ^2 = 0.1$ $\epsilon = 0.15$, Bound = 0.01 \relax }{figure.caption.30}{}}
\newlabel{fig:RBF_sig=1_E=1_B=0.6}{{2.2f}{16}{RBF : $\sigma ^2 = 1$ $\epsilon = 0.15$, Bound = 0.01 \relax }{figure.caption.30}{}}
\newlabel{sub@fig:RBF_sig=1_E=1_B=0.6}{{f}{16}{RBF : $\sigma ^2 = 1$ $\epsilon = 0.15$, Bound = 0.01 \relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces SVM for Regression using Linear, Polynomial and RBF Kernels: demo \texttt  {uiregress}. \relax }}{16}{figure.caption.30}\protected@file@percent }
\newlabel{fig:uiregress_Kernels}{{2.2}{16}{SVM for Regression using Linear, Polynomial and RBF Kernels: demo \texttt {uiregress}. \relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The sinc function}{16}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Regression of the sinc function}{16}{subsection.2.2.1}\protected@file@percent }
\newlabel{fig:Ex_1_2_1_gam(10)_sig(0.01)}{{2.3a}{17}{$\gamma = 10$ ; $\sigma ^2 = 0.01$\relax }{figure.caption.31}{}}
\newlabel{sub@fig:Ex_1_2_1_gam(10)_sig(0.01)}{{a}{17}{$\gamma = 10$ ; $\sigma ^2 = 0.01$\relax }{figure.caption.31}{}}
\newlabel{fig:Ex_1_2_1_gam(10)_sig(0.1)}{{2.3b}{17}{$\gamma = 10$ ; $\sigma ^2 = 0.1$\relax }{figure.caption.31}{}}
\newlabel{sub@fig:Ex_1_2_1_gam(10)_sig(0.1)}{{b}{17}{$\gamma = 10$ ; $\sigma ^2 = 0.1$\relax }{figure.caption.31}{}}
\newlabel{fig:Ex_1_2_1_gam(10)_sig(10)}{{2.3c}{17}{$\gamma = 10$ ; $\sigma ^2 = 10$\relax }{figure.caption.31}{}}
\newlabel{sub@fig:Ex_1_2_1_gam(10)_sig(10)}{{c}{17}{$\gamma = 10$ ; $\sigma ^2 = 10$\relax }{figure.caption.31}{}}
\newlabel{fig:Ex_1_2_1_gam(0.1)_sig(0.1)}{{2.3d}{17}{$\gamma = 0.1$ ; $\sigma ^2 = 0.1$\relax }{figure.caption.31}{}}
\newlabel{sub@fig:Ex_1_2_1_gam(0.1)_sig(0.1)}{{d}{17}{$\gamma = 0.1$ ; $\sigma ^2 = 0.1$\relax }{figure.caption.31}{}}
\newlabel{fig:Ex_1_2_1_gam(1)_sig(0.1)}{{2.3e}{17}{$\gamma = 10$ ; $\sigma ^2 = 0.1$ \relax }{figure.caption.31}{}}
\newlabel{sub@fig:Ex_1_2_1_gam(1)_sig(0.1)}{{e}{17}{$\gamma = 10$ ; $\sigma ^2 = 0.1$ \relax }{figure.caption.31}{}}
\newlabel{fig:Ex_1_2_1_gam(10000)_sig(0.1)}{{2.3f}{17}{$\gamma = 10000$ ; $\sigma ^2 = 0.1$\relax }{figure.caption.31}{}}
\newlabel{sub@fig:Ex_1_2_1_gam(10000)_sig(0.1)}{{f}{17}{$\gamma = 10000$ ; $\sigma ^2 = 0.1$\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces SVM Regressor : Function estimation for various ($\gamma $; $\sigma ^2$) values of the RBF kernel\relax }}{17}{figure.caption.31}\protected@file@percent }
\newlabel{fig:sinc_regress}{{2.3}{17}{SVM Regressor : Function estimation for various ($\gamma $; $\sigma ^2$) values of the RBF kernel\relax }{figure.caption.31}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces MSE (\%) between SVR estimations and ground truth data for various ($\gamma $ ; $\sigma ^2$)\relax }}{17}{table.caption.32}\protected@file@percent }
\newlabel{table:9}{{2.1}{17}{MSE (\%) between SVR estimations and ground truth data for various ($\gamma $ ; $\sigma ^2$)\relax }{table.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Optimal ($\gamma $, $\sigma ^2$) pairs using auto tuning methods\relax }}{17}{table.caption.33}\protected@file@percent }
\newlabel{table:10}{{2.2}{17}{Optimal ($\gamma $, $\sigma ^2$) pairs using auto tuning methods\relax }{table.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Application of the Bayesian framework}{18}{subsection.2.2.2}\protected@file@percent }
\newlabel{fig:Bayesian_1}{{2.4a}{18}{Bayes Framework: LS-SVM with ($\gamma $,$\sigma ^2$)=(10,0.4)\relax }{figure.caption.34}{}}
\newlabel{sub@fig:Bayesian_1}{{a}{18}{Bayes Framework: LS-SVM with ($\gamma $,$\sigma ^2$)=(10,0.4)\relax }{figure.caption.34}{}}
\newlabel{fig:Bayesian_2}{{2.4b}{18}{Bayes Framework: LS-SVM with ($\gamma $,$\sigma ^2$) obtained from auto tune method\relax }{figure.caption.34}{}}
\newlabel{sub@fig:Bayesian_2}{{b}{18}{Bayes Framework: LS-SVM with ($\gamma $,$\sigma ^2$) obtained from auto tune method\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Bayesian Framework : LS-SVM \relax }}{18}{figure.caption.34}\protected@file@percent }
\newlabel{fig:bayes_1}{{2.4}{18}{Bayesian Framework : LS-SVM \relax }{figure.caption.34}{}}
\newlabel{eq:2.1}{{2.1}{18}{Application of the Bayesian framework}{equation.2.2.1}{}}
\newlabel{eq:2.2}{{2.2}{18}{Application of the Bayesian framework}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Automatic Relevance Determination}{19}{section.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Data distribution\relax }}{19}{figure.caption.35}\protected@file@percent }
\newlabel{fig:Bayes_ARD_1}{{2.5}{19}{Data distribution\relax }{figure.caption.35}{}}
\newlabel{fig:Bayes_ARD_2}{{2.6a}{19}{Bayes ARD ranking for ($\gamma $;$\sigma ^2$)=(10,0.4)\relax }{figure.caption.35}{}}
\newlabel{sub@fig:Bayes_ARD_2}{{a}{19}{Bayes ARD ranking for ($\gamma $;$\sigma ^2$)=(10,0.4)\relax }{figure.caption.35}{}}
\newlabel{fig:Bayes_ARD_3}{{2.6b}{19}{Bayes ARD ranking for ($\gamma $;$\sigma ^2$) selected from cross validation\relax }{figure.caption.35}{}}
\newlabel{sub@fig:Bayes_ARD_3}{{b}{19}{Bayes ARD ranking for ($\gamma $;$\sigma ^2$) selected from cross validation\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Bayes : ARD\relax }}{19}{figure.caption.35}\protected@file@percent }
\newlabel{fig:Bayes_ARD}{{2.6}{19}{Bayes : ARD\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Robust regression}{19}{section.2.4}\protected@file@percent }
\newlabel{fig:rob_nooutliers}{{2.7a}{20}{LS-SVM Regressor : Without Outliers\relax }{figure.caption.36}{}}
\newlabel{sub@fig:rob_nooutliers}{{a}{20}{LS-SVM Regressor : Without Outliers\relax }{figure.caption.36}{}}
\newlabel{fig:rob_outliers}{{2.7b}{20}{LS-SVM Regressor : With Outliers\relax }{figure.caption.36}{}}
\newlabel{sub@fig:rob_outliers}{{b}{20}{LS-SVM Regressor : With Outliers\relax }{figure.caption.36}{}}
\newlabel{fig:rob_huber}{{2.7c}{20}{LS-SVM Regressor : Robust Cross Validation\relax }{figure.caption.36}{}}
\newlabel{sub@fig:rob_huber}{{c}{20}{LS-SVM Regressor : Robust Cross Validation\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces LS-SVM : Robust Vs Non-Robust versions\relax }}{20}{figure.caption.36}\protected@file@percent }
\newlabel{fig:robust}{{2.7}{20}{LS-SVM : Robust Vs Non-Robust versions\relax }{figure.caption.36}{}}
\newlabel{fig:rob_nooutliers}{{2.8a}{20}{Robust LS-SVM Regressor : Logistic\relax }{figure.caption.37}{}}
\newlabel{sub@fig:rob_nooutliers}{{a}{20}{Robust LS-SVM Regressor : Logistic\relax }{figure.caption.37}{}}
\newlabel{fig:rob_myriad}{{2.8b}{20}{Robust LS-SVM Regressor : Myriad\relax }{figure.caption.37}{}}
\newlabel{sub@fig:rob_myriad}{{b}{20}{Robust LS-SVM Regressor : Myriad\relax }{figure.caption.37}{}}
\newlabel{fig:rob_hampel}{{2.8c}{20}{Robust LS-SVM Regressor : Hampel\relax }{figure.caption.37}{}}
\newlabel{sub@fig:rob_hampel}{{c}{20}{Robust LS-SVM Regressor : Hampel\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces LS-SVM : Robust versions with different weighting functions\relax }}{20}{figure.caption.37}\protected@file@percent }
\newlabel{fig:robust_versions}{{2.8}{20}{LS-SVM : Robust versions with different weighting functions\relax }{figure.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces LS-SVM Robust Regressor with different weighting functions and their optimal hyper parameters ($\gamma $;$\sigma ^2$)\relax }}{21}{table.caption.38}\protected@file@percent }
\newlabel{table:11}{{2.3}{21}{LS-SVM Robust Regressor with different weighting functions and their optimal hyper parameters ($\gamma $;$\sigma ^2$)\relax }{table.caption.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Homework Problems}{21}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Logmap dataset}{21}{subsection.2.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces LS-SVM: Logmap Dataset with ($\gamma $;$\sigma ^2$)=(10;10)\relax }}{21}{figure.caption.39}\protected@file@percent }
\newlabel{fig:hw1}{{2.9}{21}{LS-SVM: Logmap Dataset with ($\gamma $;$\sigma ^2$)=(10;10)\relax }{figure.caption.39}{}}
\newlabel{fig:hw_tuned1}{{2.10a}{21}{LogMap: LS-SVM Regressor with optimal ($\gamma $;$\sigma ^2$)=(2702.4514 ; 86.8411), lag = 23 and MAE = 12.09(\%)\relax }{figure.caption.40}{}}
\newlabel{sub@fig:hw_tuned1}{{a}{21}{LogMap: LS-SVM Regressor with optimal ($\gamma $;$\sigma ^2$)=(2702.4514 ; 86.8411), lag = 23 and MAE = 12.09(\%)\relax }{figure.caption.40}{}}
\newlabel{fig:hw_tuned2}{{2.10b}{21}{LogMap : LS-SVM Regressor with optimal ($\gamma $;$\sigma ^2$)=(11029.3131 ; 269.0996), lag = 23 and MAE = 12.5799(\%)\relax }{figure.caption.40}{}}
\newlabel{sub@fig:hw_tuned2}{{b}{21}{LogMap : LS-SVM Regressor with optimal ($\gamma $;$\sigma ^2$)=(11029.3131 ; 269.0996), lag = 23 and MAE = 12.5799(\%)\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces LS-SVM: Logmap Dataset with optimally tuned hyper parameters\relax }}{21}{figure.caption.40}\protected@file@percent }
\newlabel{fig:hw2}{{2.10}{21}{LS-SVM: Logmap Dataset with optimally tuned hyper parameters\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Santa Fe dataset}{22}{subsection.2.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Santa Fe Dataset: ($\gamma $;$\sigma ^2$)=(10,10) and Lag = 50\relax }}{22}{figure.caption.41}\protected@file@percent }
\newlabel{fig:sfe1}{{2.11}{22}{Santa Fe Dataset: ($\gamma $;$\sigma ^2$)=(10,10) and Lag = 50\relax }{figure.caption.41}{}}
\newlabel{fig:sfe2}{{2.12a}{22}{Santa Fe Dataset: ($\gamma $;$\sigma ^2$) = (5709.0283 ; 10.6468) and MAE = 30.6207(\%)\relax }{figure.caption.42}{}}
\newlabel{sub@fig:sfe2}{{a}{22}{Santa Fe Dataset: ($\gamma $;$\sigma ^2$) = (5709.0283 ; 10.6468) and MAE = 30.6207(\%)\relax }{figure.caption.42}{}}
\newlabel{fig:sfe3}{{2.12b}{22}{Santa Fe Dataset : MAPE for optimal ($\gamma $;$\sigma ^2$) over a range of order/lag values\relax }{figure.caption.42}{}}
\newlabel{sub@fig:sfe3}{{b}{22}{Santa Fe Dataset : MAPE for optimal ($\gamma $;$\sigma ^2$) over a range of order/lag values\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Santa Fe Dataset: Best Fit with optimal hyper parameters and corresponding MAPE for the tuned ($\gamma $;$\sigma ^2$) pairs\relax }}{22}{figure.caption.42}\protected@file@percent }
\newlabel{fig:sfe}{{2.12}{22}{Santa Fe Dataset: Best Fit with optimal hyper parameters and corresponding MAPE for the tuned ($\gamma $;$\sigma ^2$) pairs\relax }{figure.caption.42}{}}
\@setckpt{Exercise2/Report/Exercise2}{
\setcounter{page}{23}
\setcounter{equation}{2}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{12}
\setcounter{table}{3}
\setcounter{float@type}{8}
\setcounter{FBl@b}{0}
\setcounter{FRobj}{0}
\setcounter{FRsobj}{0}
\setcounter{FBcnt}{0}
\setcounter{blindtext}{1}
\setcounter{Blindtext}{5}
\setcounter{blind@countparstart}{0}
\setcounter{blindlist}{0}
\setcounter{blindlistlevel}{0}
\setcounter{blindlist@level}{0}
\setcounter{blind@listcount}{0}
\setcounter{blind@levelcount}{0}
\setcounter{blind@randomcount}{0}
\setcounter{blind@randommax}{0}
\setcounter{blind@pangramcount}{0}
\setcounter{blind@pangrammax}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{2}
\setcounter{subtable}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{19}
\setcounter{FBLTpage}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{0}
}
